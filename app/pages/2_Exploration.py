import os

import streamlit as st
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from routes import redirection
from sklearn.impute import SimpleImputer
from modules.mod_exploration import (combine_data_sources, print_api_data, encoder_cible,
                                     detection_outliers, traiter_outliers, standardisation,
                                     telecharger_donnees, generer_rapport_pdf
                                    )

# Initialize connection.
conn = st.connection("postgresql", type="sql")
references = st.secrets["filename"]

# En-tÃªte principal
st.markdown("""
    <div class="main-header">
        <h1>ğŸ” Exploration des donnÃ©es</h1>
        <p style="font-size: 1.2em; margin-top: 1rem;">
            Exploration des donnÃ©es
        </p>
    </div>
    """, unsafe_allow_html=True)

# RÃ©cupÃ©ration du DF
# if "df" not in st.session_state:
#     st.warning("âŒ Veuiller importer des donnÃ©es pour pouvoir explorer.")
#     st.stop()
# else:
#     df = st.session_state["df"]

df = combine_data_sources()
with st.expander("Voir les donnÃ©es combinÃ©es", expanded=False):
    st.dataframe(df)

df_api = print_api_data()
with st.expander("DonnÃ©es de l'API", expanded=False):
    st.dataframe(df_api)

# Onglets de navigation
onglet1, onglet2, onglet3, onglet4, onglet5, onglet6 = st.tabs([
    "ğŸ§¬ Types & TÃ¢che",
    "ğŸ“Š Distributions",
    "ğŸ“ˆ CorrÃ©lations",
    "ğŸ§¹ NaN & Outliers",
    "âš–ï¸ Standardisation",
    "ğŸ¯ RÃ©sumÃ© & Exports"
])

### Onglet 1 - Types & Cible
with onglet1:

    with st.expander("â„¹ï¸ Fonctionnement"):
        st.info("""
        Cette section permet d'identifier les types de chaque variable (numÃ©rique, catÃ©gorielle...) et de choisir la variable cible pour la modÃ©lisation. \n
        En fonction de la cible choisie, l'application propose automatiquement un type de tÃ¢che : **classification** (si 10 modalitÃ©s ou moins) ou **rÃ©gression** (valeurs continues).\n
        Vous pouvez forcer ce choix manuellement.
        """)
    
    # Affichage du type de donnÃ©es pour chaque colonne
    st.subheader("ğŸ§¬ Types de donnÃ©es")
    st.dataframe(df.dtypes.reset_index().rename(columns={0: "Type", "index": "Colonne"}))

    st.write("***")
    
    # L'utilisateur choisit sa colonne cible
    st.subheader("ğŸ¯ SÃ©lection de la variable cible")
    target = st.selectbox("Choisissez la colonne cible", df.columns)

    st.write("***")
    
    st.info(f"Si la cible comporte jusqu'Ã  10 valeurs diffÃ©rente, on estime que c'est de la Classification sinon RÃ©gression")

    st.subheader("ğŸ§  Type de tÃ¢che")
    # AutodÃ©tection
    # nunique() compte le nombre de valeurs uniques dans la colonne target
    auto_detected_task = "classification" if df[target].nunique() <= 10 else "rÃ©gression"
    st.write(f"Suggestion automatique : **{auto_detected_task}**")

    # Choix par l'utilisateur avec le choix autodÃ©tectÃ© par dÃ©faut
    task = st.radio("Choisissez le type de tÃ¢che",
                    ["classification", "rÃ©gression"], index=0 
                    if auto_detected_task == "classification" else 1
                    )
    st.success(f"Type de tÃ¢che sÃ©lectionnÃ© : **{task}**")
    st.session_state['task']=task

    st.write("***")
    st.markdown("### Vous pouvez maintenant passer Ã  l'onglet suivant : ğŸ“Š Distributions")

### Onglet 2 - Distributions
with onglet2:
    st.subheader("ğŸ“Š Distribution des variables")
    
    with st.expander("â„¹ï¸ Fonctionnement"):
        st.info("""
        Cet onglet permet d'explorer la distribution des valeurs d'une variable :
        - Pour les colonnes **numÃ©riques**, un histogramme avec une courbe de densitÃ© (KDE) montre la forme de la distribution (normale, asymÃ©trique, etc).
        - Pour les colonnes **catÃ©gorielles**, un diagramme Ã  barres indique la frÃ©quence de chaque modalitÃ©.

        Cela aide Ã  dÃ©tecter les valeurs aberrantes, les dÃ©sÃ©quilibres ou les transformations Ã  appliquer avant le traitement.
        """)

    col1, col2, col3 = st.columns([1, 2, 1])  # colonne centrale plus large pour le graphe
    with col1:
        # SÃ©lection de la colonne Ã  analyser
        selected_col = st.selectbox("SÃ©lectionnez une colonne", df.columns)
    with col2:
        # Si la colonne est de type numÃ©rique : histplot + kde
        if pd.api.types.is_numeric_dtype(df[selected_col]):
            fig, ax = plt.subplots(figsize=(6, 4))
            sns.histplot(df[selected_col].dropna(), kde=True, ax=ax)
            st.pyplot(fig)
        else:
            # Sinon, barplot
            counts = df[selected_col].value_counts()
            fig, ax = plt.subplots(figsize=(6, 4))
            sns.barplot(x=counts.index, y=counts.values, ax=ax)
            for i, v in enumerate(counts.values):
                ax.text(i, v, str(v), ha='center', va='bottom')
            plt.xticks(rotation=45)
            st.pyplot(fig)
            
    st.write("***")
    st.markdown("### Vous pouvez maintenant passer Ã  l'onglet suivant : ğŸ“ˆ CorrÃ©lations")

### Onglet 3 - CorrÃ©lations
with onglet3:
    with st.expander("â„¹ï¸ Fonctionnement"):
        st.info("""
        Cette section permet de visualiser les relations entre les variables numÃ©riques grÃ¢ce Ã  une matrice de corrÃ©lation.\n
        Elle aide Ã  dÃ©tecter les variables redondantes ou trÃ¨s corrÃ©lÃ©es, qu'il peut Ãªtre utile d'exclure pour Ã©viter les multicolinÃ©aritÃ©s.\n
        Elle permet Ã©galement de sÃ©lectionner les colonnes Ã  conserver pour la modÃ©lisation, en se basant sur cette matrice.
        """)

    # ENCODAGE Si Classification et target non numÃ©rique
    df_corr = st.session_state.get("df_corr", df.copy())
    target_corr = st.session_state.get("target_corr", target)

    if task == "classification" and not pd.api.types.is_numeric_dtype(df_corr[target]):
        # Si on est sur une classification, l'utilisateur choisit entre 3 mÃ©thodes d'encodage
        choix_encoder = st.selectbox("Encodage de la cible", ["Label Encoding", "One-Hot Encoding", "get_dummies"])
        if choix_encoder == "Label Encoding":
            drop_first = False
        else:
            drop_first = st.checkbox("Supprimer la premiÃ¨re modalitÃ© (Ã©vite la multicolinÃ©aritÃ©)", value=False)

        # Application de l'encodage choisi
        if st.button("Appliquer l'encodage"):
            df_corr = df.copy()
            df_corr, encoded_target_name = encoder_cible(df_corr, target, choix_encoder, drop_first)
            df_corr = df_corr.loc[:, ~df_corr.columns.duplicated()]
            st.session_state["df_corr"] = df_corr
            st.session_state["target_corr"] = encoded_target_name
            st.session_state["choix_encoder"] = choix_encoder
            st.success("Encodage appliquÃ© avec succÃ¨s.")
            target_corr = encoded_target_name

        # Affichage de l'encoder appliquÃ© actuellement
        if "choix_encoder" in st.session_state:
            st.info(f"Encodage actuellement appliquÃ© : `{st.session_state['choix_encoder']}`")
    else:
        st.session_state["df_corr"] = df_corr
        st.session_state["target_corr"] = target_corr

    numeric_cols = df_corr.select_dtypes(include=np.number).columns.tolist()

    target_cols = st.session_state.get("target_corr")

    if target_cols is None:
        st.warning("Aucune variable cible sÃ©lectionnÃ©e.")
        st.stop()

    target_cols = target_cols if isinstance(target_cols, list) else [target_cols]
    cols_to_include = [col for col in numeric_cols if col in df_corr.columns]
    corr = df_corr[cols_to_include].corr()

    st.subheader("ğŸ”¢ AperÃ§u des donnÃ©es")
    st.dataframe(df_corr.sample(n=10))

    ### MATRICE DE CORRELATION
    st.subheader("ğŸ“ˆ Matrice de corrÃ©lation")

    fig, ax = plt.subplots(figsize=(10, 8))
    sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm", ax=ax)
    st.pyplot(fig)

    st.write("*"*10)

    # Si rÃ©gression, on conserve les colonnes ayant une corrÃ©lation >0.2 avec la target sinon >0.1
    seuil_bas = 0.2 if task == "rÃ©gression" else 0.1
    # Dans les deux cas, on conserve les colonnes ayant une corrÃ©lation <0.95 avec la target
    seuil_haut = 0.95

    st.subheader(f"ğŸ“‰ Colonnes pas assez corrÃ©lÃ©es (< {seuil_bas})")
    to_drop = []
    for col in corr.columns:
        if col != target_corr:
            val_corr = corr.at[col, target_corr]
            if abs(val_corr) < seuil_bas:
                to_drop.append({"Colonne": col, "CorrÃ©lation avec la cible": corr.at[col, target_corr]})

    if to_drop:
        df_to_drop = pd.DataFrame(to_drop).sort_values(by="CorrÃ©lation avec la cible")
        st.dataframe(df_to_drop, hide_index=True)
    else:
        st.write("Pas de colonne faiblement corrÃ©lÃ©e Ã  la cible")

    st.subheader(f"ğŸ“‰ Colonnes trop corrÃ©lÃ©es (> {seuil_haut})")
    corr_trop_fortes = []
    for i in range(len(corr.columns)):
        for j in range(i):
            val_corr = corr.iloc[i, j]
            if abs(val_corr) > seuil_haut:
                corr_trop_fortes.append({
                    "Colonne 1": corr.columns[i],
                    "Colonne 2": corr.columns[j],
                    "CorrÃ©lation": round(val_corr, 3)
                })

    if corr_trop_fortes:
        df_corr_fortes = pd.DataFrame(corr_trop_fortes).sort_values(by="CorrÃ©lation", ascending=False)
        st.dataframe(df_to_drop, hide_index=True)
    else:
        st.write("Pas de colonne trop corrÃ©lÃ©e")

    st.write("*"*10)

    st.subheader("âœ… SÃ©lection guidÃ©e des colonnes")

    # VÃ©rifier que toutes les colonnes cible sont bien dans la matrice
    if isinstance(target_corr, list):
        cibles_valides = [c for c in target_corr if c in corr.columns]
        if not cibles_valides:
            st.warning("Aucune colonne cible encodÃ©e n'est prÃ©sente dans la matrice de corrÃ©lation.")
            good_corr_cols = []
        else:
            # AgrÃ©gation des corrÃ©lations sur les colonnes cibles
            corr_scores = pd.Series(0, index=corr.columns)
            for col in cibles_valides:
                corr_scores += abs(corr[col])
            corr_scores /= len(cibles_valides)

            filtres_target = (corr_scores > seuil_bas) & (corr_scores <= seuil_haut)
            good_corr_cols = corr_scores[filtres_target].drop(labels=cibles_valides, errors="ignore").index.tolist()
    else:
        if target_corr not in corr.columns:
            st.warning(f"La colonne cible '{target_corr}' n'est plus prÃ©sente dans les donnÃ©es.")
            good_corr_cols = []

        else:
            # Si rÃ©gression, on conserve les colonnes ayant une corrÃ©lation >0.2 avec la target
            if task == "rÃ©gression":
                seuil_bas = 0.2
            # Si classification, on conserve les colonnes ayant une corrÃ©lation >0.1 avec la target
            else:
                seuil_bas = 0.1

            # Colonnes corrÃ©lÃ©es Ã  la target entre 0.1 ou 0.2 et 0.95
            filtres_target = (abs(corr[target_corr]) > seuil_bas) & (abs(corr[target_corr]) <= 0.95)
            good_corr_cols = corr[target_corr][filtres_target].index.tolist()

        # Ã‰limination des colonnes trop corrÃ©lÃ©es entre elles (>0.95)
        if good_corr_cols:
            corr_matrix = df[good_corr_cols].corr().abs()
            upper_triangle = corr_matrix.where(
                np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
            )

            to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.95)]
            good_corr_cols = [col for col in good_corr_cols if col not in to_drop]
        else:
            to_drop=[]

    selected_cols_corr = st.multiselect("Colonnes Ã  conserver pour la modÃ©lisation", numeric_cols, default=good_corr_cols)

    # # initialiser df_clean dans le session_state pour l'onglet 4
    if st.button("âœ… Valider cette version comme jeu de donnÃ©es nettoyÃ©"):
        df_corr_validated = df_corr[selected_cols_corr + [target_corr]].copy()
        st.session_state["df_clean"] = df_corr_validated
        st.success("La version avec encodage a Ã©tÃ© validÃ©e comme jeu de donnÃ©es nettoyÃ© (df_clean).")

        st.write("*"*10)
        st.markdown("### Vous pouvez maintenant passer Ã  l'onglet suivant : ğŸ§¹ NaN & Outliers")

# Onglet 4 - NaN & Outliers
with onglet4:
    if "df_clean" in st.session_state:
        df=st.session_state["df_clean"].copy()

        with st.expander("â„¹ï¸ Fonctionnement"):
            st.info("""
            Cet onglet permet :
            - d'analyser les valeurs manquantes (NaN) et de choisir une mÃ©thode d'imputation (moyenne, mÃ©diane, valeur la plus frÃ©quente)
            - de dÃ©tecter les valeurs aberrantes (outliers), afin de les supprimer ou ajuster au besoin.
            """)

        # Gestion des NaN
        st.subheader("ğŸ§¹ Gestion des NaN")
        nan_summary = df.isna().sum()
        st.dataframe(nan_summary[nan_summary > 0])

        selected_nan = st.multiselect("Colonnes Ã  traiter", nan_summary[nan_summary > 0].index.tolist())
        imputation_label = st.selectbox("MÃ©thode d'imputation", ["La Moyenne", "La MÃ©diane", "La valeur la plus frÃ©quente"])
        translate = {
            "La Moyenne":"mean",
            "La MÃ©diane":"median",
            "La valeur la plus frÃ©quente":"most_frequent"
        }
        imputation_strategy = translate[imputation_label]

        if st.button("Remplacer les NaN"):
            if not selected_nan:
                st.warning("Veuillez sÃ©lectionner au moins une colonne Ã  traiter.")
            else:
                imputer = SimpleImputer(strategy=imputation_strategy)
                df[selected_nan] = pd.DataFrame(
                        imputer.fit_transform(df[selected_nan]),
                        columns=selected_nan,
                        index=df.index
                    )
                st.success("NaN remplacÃ©s avec succÃ¨s")
                st.session_state["df_clean"] = df

        st.write("*"*10)

        # Gestion des  Outliers
        st.subheader("ğŸš¨ DÃ©tection des outliers")

        # L'utilisateur choisit la mÃ©thode de dÃ©tection des outliers
        methode_outlier = st.radio("MÃ©thode de dÃ©tection", ["Z-score", "IQR"])
        st.info("""Z-Score est une technique statistique qui standardise les points de donnÃ©es.\n
                IQR (Interquartile Range) utilise les quartiles pour identifier les valeurs aberrantes.
                """) 

        numeric_cols = df.select_dtypes(include=np.number).columns.tolist()  # recalcul aprÃ¨s potentielle suppression de colonnes
        outliers_bool = detection_outliers(df, numeric_cols, methode_outlier, key="zscore1")

        st.write("Nombre d'outliers par colonne :")
        st.write(outliers_bool.sum())

        traitement_outlier = st.selectbox("Action", ["Aucune", "Supprimer les lignes", "Remplacer par la mÃ©diane"])

        if st.button("Remplacer les Outliers"):
            if traitement_outlier != "Aucune":
                df = traiter_outliers(df, numeric_cols, outliers_bool, traitement_outlier)
                st.success("Traitement des outliers effectuÃ©.")
                st.session_state["df_clean"] = df
                numeric_cols = df.select_dtypes(include=np.number).columns.tolist()
                outliers_bool_clean = detection_outliers(df, numeric_cols, methode_outlier, key="zscore2")
                st.write(outliers_bool_clean.sum())

        st.write("***")
        st.markdown("### Vous pouvez maintenant passer Ã  l'onglet suivant : âš–ï¸ Standardisation")

# Onglet 5 - Standardisation
with onglet5:
    if "df_clean" in st.session_state:
        df_clean = st.session_state["df_clean"].copy()
        
        with st.expander("â„¹ï¸ Fonctionnement"):
            st.info("""
                Cette onglet permet de standardiser les donnÃ©es afin que certaines colonnes n'aient pas un poids supÃ©rieur Ã  d'autres dans la modÃ©lisation.\n
                Il vous sera indiquÃ© si vos donnÃ©es semblent dÃ©jÃ  standardisÃ©es ou non et si ce n'est pas le cas, vous pourrez cocher la case pour le faire.
            """)
            
        colonne_target = st.session_state["target_corr"]
        
        st.subheader("âš–ï¸ Standardisation des donnÃ©es")
        standardisation(df_clean, colonne_target)    
        
        st.write("***")
        st.markdown("### Vous pouvez maintenant passer Ã  l'onglet suivant : ğŸ¯ RÃ©sumÃ© & Exports")

# Onglet 6 - RÃ©sumÃ© & Exports
with onglet6:
    if "df_clean" in st.session_state:
        df_clean = st.session_state["df_clean"].copy()
        
        with st.expander("â„¹ï¸ Fonctionnement"):
            st.info("""
            Cette onglet permet :
            - d'afficher un rÃ©sumÃ© des observations et traitements rÃ©alisÃ©s,
            - de tÃ©lÃ©charger les donnÃ©es au format CSV ou XLSX,
            - de gÃ©nÃ©rer et tÃ©lÃ©charger un rapport d'exploration et traitement au format PDF.
            """)
        
        # RÃ©sumÃ© des traitements effectuÃ©s
        st.subheader("ğŸ“Œ RÃ©sumÃ© des traitements effectuÃ©s")

        # 1. Nombre de lignes et colonnes
        n_lignes, n_colonnes = df_clean.shape

        # 2. Colonnes supprimÃ©es pour forte corrÃ©lation
        nb_col_corr_suppr = len(to_drop)

        # 3. Colonnes conservÃ©es
        nb_colonnes_conservees = len(selected_cols_corr)

        # 4. Type de tÃ¢che
        type_tache_resume = task

        # 5. MÃ©thode d'encodage
        encoder_resume = st.session_state.get("choix_encoder", "Aucun encodage (cible numÃ©rique)")

        # Affichage
        st.markdown(f"""
        - âœ… **{n_lignes} lignes** et **{n_colonnes} colonnes** finales
        - ğŸ§  **Type de tÃ¢che** sÃ©lectionnÃ© : **{type_tache_resume}**
        - ğŸ¯ **Variable cible encodÃ©e avec** : `{encoder_resume}`
        - ğŸ§¹ **Colonnes supprimÃ©es pour forte corrÃ©lation** (> 0.95) : {nb_col_corr_suppr}
        - ğŸ“Š **Nombre de colonnes conservÃ©es** pour la modÃ©lisation : {nb_colonnes_conservees}
        """)
        if st.session_state.get("standardized", False):
            nb_std_cols = len(st.session_state.get("standardized_columns", []))
            st.markdown(f"- âš–ï¸ **Standardisation appliquÃ©e sur {nb_std_cols} colonnes**")
            with st.expander("ğŸ” DÃ©tail des colonnes standardisÃ©es (moyenne et Ã©cart-type)"):
                st.dataframe(st.session_state.get("standardized_stats", pd.DataFrame()))
        else:
            st.markdown("- âš–ï¸ **Aucune standardisation appliquÃ©e**")

        st.write("***")
        
        # TÃ©lÃ©chargement des donnÃ©es au format CSV ou XLSX
        telecharger_donnees(df_clean)
        
        st.write("***")
        
        # TÃ©lÃ©chargement d'un Rapport d'explo en PDF
        st.subheader("ğŸ“ GÃ©nÃ©rer un rapport de l'exploration et des traitements en PDF")

        if st.button("ğŸ“„ GÃ©nÃ©rer le rapport PDF"):
            pdf_path, images = generer_rapport_pdf(df, df_clean, target_corr, selected_cols_corr, task, to_drop, corr)

            with open(pdf_path, "rb") as f:
                st.download_button(
                    label="ğŸ“„ TÃ©lÃ©charger le rapport PDF",
                    data=f.read(),
                    file_name="rapport_exploration.pdf",
                    mime="application/pdf"
                )

            try:
                os.remove(pdf_path)
                for img in images:
                    os.remove(img)
            except Exception as e:
                st.warning(f"Erreur lors de la suppression des fichiers temporaires : {e}")
        
        # Redirection page suivante
        # redirection("ğŸ¦¾ EntraÃ®nement d'un modÃ¨le", "3_Machine Learning")

import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    mean_squared_error,
    r2_score,
)


def evaluation_classification(model, X_test, y_test):
    y_pred = model.predict(X_test)

    # Rapport sous forme de DataFrame
    rapport = classification_report(y_test, y_pred, output_dict=True)
    df_rapport = pd.DataFrame(rapport).transpose().round(2)
    # Affichage stylis√©
    st.dataframe(
        df_rapport.style.background_gradient(cmap="Blues", axis=1).format(precision=2)
    )

    cm = confusion_matrix(y_test, y_pred)
    labels = sorted(set(y_test))
    fig, ax = plt.subplots()
    sns.heatmap(
        cm,
        annot=True,
        fmt="d",
        cmap="Blues",
        xticklabels=labels,
        yticklabels=labels,
        ax=ax,
    )
    ax.set_xlabel("Valeurs pr√©dites")
    ax.set_ylabel("Valeurs r√©elles")
    ax.set_title("Matrice de confusion")

    col1, col2, col3 = st.columns(
        [2, 1, 1]
    )  # colonne centrale plus large pour le graph
    with col1:
        st.pyplot(fig)


def evaluation_regression(model, X_test, y_test):
    y_pred = model.predict(X_test)

    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)

    st.markdown("### üìà R√©sultats de la r√©gression")
    st.markdown(f"- **RMSE** : `{rmse:.2f}`")
    st.markdown(f"- **R¬≤** : `{r2:.2f}`")

    fig, ax = plt.subplots()
    ax.scatter(y_test, y_pred, alpha=0.6)
    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], "r--")
    ax.set_xlabel("Valeurs r√©elles")
    ax.set_ylabel("Pr√©dictions")
    ax.set_title("Valeurs r√©elles vs. Pr√©dictions")
    st.pyplot(fig)


def afficher_evaluation(model, X_test, y_test, type_tache):
    if type_tache == "classification":
        evaluation_classification(model, X_test, y_test)
    else:
        evaluation_regression(model, X_test, y_test)


def fonctionnement(type_tache):
    if type_tache == "classification":
        st.markdown("### üìã Rapport de Classification")
        with st.expander("‚ÑπÔ∏è Fonctionnement"):
            st.info(
                """
                Cette section affiche des m√©triques d'√©valuation du mod√®le. Ces m√©triques seront diff√©rentes en fonction du type de t√¢che (Classification ou R√©gression) effectu√©e.\n
                Nous sommes ici sur de la **Classification**.\n
                - Interpr√©tation des lignes :\n
                    * 0, 1, 2... repr√©sentent les **Classes Pr√©dictives**\n
                    * accuracy repr√©sente l‚Äô**Exactitude Globale** : proportion totale de bonnes pr√©dictions\n
                    * macro avg	repr√©sente la **Moyenne Non Pond√©r√©e** des m√©triques par classe\n
                    * weighted avg repr√©sente la **Moyenne Pond√©r√©e** par le support (nombre d‚Äôexemples de chaque classe)\n
                - Interpr√©tation des colonnes :\n
                    * precision repr√©sente le nombre de pr√©dictions correctes parmi les pr√©dictions positives pour cette classe,\n
                    * recall montre combien des vrais √©l√©ments de cette classe ont √©t√© bien retrouv√©s,\n
                    * f1-score repr√©sente la **Moyenne Harmonique** entre pr√©cision et rappel (√©quilibre entre les deux)\n
                    * le support affiche le nombre de cas r√©els pour chaque classe
            """
            )
    else:
        st.markdown("### üìã Rapport de R√©gression")
        with st.expander("‚ÑπÔ∏è Fonctionnement"):
            st.info(
                """
                Cette section affiche des m√©triques d'√©valuation du mod√®le. Ces m√©triques seront diff√©rentes en fonction du type de t√¢che (Classification ou R√©gression) effectu√©e.\n
                Nous sommes ici sur de la **R√©gression**.\n
                Interpr√©tation des m√©triques :
                    * RMSE (Root Mean Squared Error) : Racine de la moyenne des carr√©s des erreurs\n
                        Plus bas = meilleures pr√©dictions (en unit√©s de la variable cible)
                    * MAE (Mean Absolute Error) : Moyenne des erreurs absolues\n
                        Plus bas = erreurs faibles, moins sensibles aux outliers
                    * R¬≤ (Coefficient de d√©termination) : Proportion de variance expliqu√©e par le mod√®le\n
                        1.0 = parfait, 0 = pire qu'une moyenne constante, peut √™tre < 0 si catastrophique
                    """
            )
